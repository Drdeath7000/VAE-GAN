{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bw6H1yVgkkMX"
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\given}[]{\\,\\middle\\vert\\,}\n",
    "\\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)}\n",
    "\\newcommand{\\grad}[]{\\nabla}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "$$\n",
    "\n",
    "# Part 3: Summary Questions\n",
    "<a id=part2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lK_HZZhzkkMa"
   },
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCs6Wgc6kkMa"
   },
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMQRH-WJkkMa"
   },
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Taxqfb_0ooDY"
   },
   "source": [
    "**Answer** The receptive field in Convolutional Neural Networks (CNN) is the region of the input space that affects a particular unit of the network. Note that this input region can be not only the input of the network but also output from other units in the network, therefore this receptive field can be calculated relative to the input that we consider and also relative the unit that we are taking into consideration as the “receiver” of this input region. Usually, when the receptive field term is mentioned, it is taking into consideration the final output unit of the network (i.e. a single unit on a binary classification task) in relation to the network input (i.e. input image of the network).\n",
    "\n",
    "Link:\n",
    "https://blog.christianperone.com/2017/11/the-effective-receptive-field-on-cnns/#:~:text=The%20receptive%20field%20in%20Convolutional,particular%20unit%20of%20the%20network.&text=The%20numbers%20inside%20the%20pixels,sliding%20step%20of%20the%20filter).\n",
    "\n",
    "Receptive fields are defined portion of space or spatial construct containing units that provide input to a set of units within a corresponding layer.\n",
    "The receptive field is defined by the filter size of a layer within a convolution neural network. The receptive field is also an indication of the extent of the scope of input data a neuron or unit within a layer can be exposed to (see image below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsI-74WJkkMb"
   },
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AjSrgiksIVk"
   },
   "source": [
    "**Answer**\n",
    "\n",
    "1. Change stride: changing stride can apply the kernel on feature inputs more or less. For example increasing the stride will cause the same feature to be a part of a calculation less.\n",
    "\n",
    "2. Use polling: polling groups togther a patch of feature maps into one cell, for example max pooling will take the maximum value of the patch and keep it as a single value in the next feature map.\n",
    "\n",
    "3. Use dilation: applies the kernel of non negihboring inputs. This mixes inputs from diffrent location in the previous feature map.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLFOWJs5kkMb"
   },
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:16.753322Z",
     "iopub.status.busy": "2021-01-26T09:18:16.752610Z",
     "iopub.status.idle": "2021-01-26T09:18:17.987501Z",
     "shell.execute_reply": "2021-01-26T09:18:17.988034Z"
    },
    "id": "c0fE3_aZkkMb",
    "outputId": "8d14978a-f51b-483b-fdf1-370b36955cac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 122, 122])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeZ7SpxVkkMd"
   },
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVNNifcPokDG"
   },
   "source": [
    "**Answer**: 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vowLf28JkkMd"
   },
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y8qBzJ8GxDr"
   },
   "source": [
    "**Answer**: During the time of backpropagation, there are two pathways for the gradients to transit back to the input layer while traversing a residual block.  We have gradient pathway-1 and gradient pathway-2. When the computed gradients pass from the Gradient Pathway-2, two weight layers are encountered in our residual function F(x). The weights or the kernels in the weight layers  are updated and new gradient values are calculated. In the case of initial layers, the newly computed values will either become small or eventually vanish. To save the gradient values from vanishing, the shortcut connection (identity mapping) will come into the picture. The gradients can directly pass through the Gradient Pathway-1. In Gradient Pathway-1, the gradients don’t have to encounter any weight layer, hence, there won’t be any change in the value of computed gradients. The residual block will be skipped at once and the gradients can reach the initial layers which will help them to learn the correct weights. Which adjusts the input layer to increase the performance of the network. As well as it makes use of the Identity Connection, which helps to protect the network from vanishing gradient problem, and it uses bottleneck residual block design to increase the performance of the network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2TtS2q0kkMd"
   },
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2ZqLvrXkkMe"
   },
   "source": [
    "1. Consider the following neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:17.992615Z",
     "iopub.status.busy": "2021-01-26T09:18:17.991868Z",
     "iopub.status.idle": "2021-01-26T09:18:18.013482Z",
     "shell.execute_reply": "2021-01-26T09:18:18.014164Z"
    },
    "id": "e8E4t8VckkMe",
    "outputId": "a5d6ab64-1a41-4bfd-f9db-1c570671e53a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Dropout(p=0.1, inplace=False)\n",
       "  (3): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "p1, p2 = 0.1, 0.2\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=p1),\n",
    "    nn.Dropout(p=p2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggc0QwR6kkMe"
   },
   "source": [
    "If we want to replace the two consecutive dropout layers with a single one defined as follows:\n",
    "```python\n",
    "nn.Dropout(p=q)\n",
    "```\n",
    "what would the value of `q` need to be? Write an expression for `q` in terms of `p1` and `p2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7NIVmFfHcuA"
   },
   "source": [
    "**Answer**: q = 1 - ((1-P1)*(1-P2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHALUTbLkkMe"
   },
   "source": [
    "2. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-BF5lOePAfq"
   },
   "source": [
    "**Answer**: True, There’s some debate as to whether the dropout should be placed before or after the activation function. As a rule of thumb, place the dropout after the activate function for all activation functions other than relu. In passing 0.5, every hidden unit (neuron) is set to 0 with a probability of 0.5. In other words, there’s a 50% change that the output of a given neuron will be forced to 0.\n",
    "At much lower levels: p=0.1 or 0.2. Dropout was used after the activation function of each convolutional layer: CONV->RELU->DROP. You apply dropout after the non-linear activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3-mMK3SkkMe"
   },
   "source": [
    "3. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Let N be the number of neurons in our network.\n",
    "Then N * (1-p) is the amount of neurons in our network after applying the dropout.\n",
    "Therefore, since we have less neurons, and the sum of weights of all neurons is lower, each of the neurons that are left will have a higher relative weight, it's new weight relative to it's previous one is now **1-p** times higher.\n",
    "So we have to scale these activations by 1 / (1-p) to bring their relative weight back to it's original expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdkAZdhXkkMf"
   },
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60HINmJUkkMf"
   },
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_S2aO5rHmuTH"
   },
   "source": [
    "Answer: Classification seeks to predict a value from a finite set of categories, and since the quadratic loss function gives a measure of how accurate a predictive model is, thus it is used on classification schemes which produce probabilities, in addition, we can use a least squares loss function to\n",
    "measure the quality of any particular weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3khQ2mg5kkMf"
   },
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://sparrowism.soc.srcf.net/home/piratesarecool4.gif\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.019108Z",
     "iopub.status.busy": "2021-01-26T09:18:18.018447Z",
     "iopub.status.idle": "2021-01-26T09:18:18.042969Z",
     "shell.execute_reply": "2021-01-26T09:18:18.043669Z"
    },
    "id": "oYR-XwGJkkMf"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H), nn.Sigmoid(),\n",
    "    ]*24,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eBONm4kkkMf"
   },
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJKTmqqDtF57"
   },
   "source": [
    "Answer: Since this is a sequential function, it will add more layers using sigmoid activation function, however as more layers are being added to our neural network, the gradients of the loss function approaches zero, making the network hard to train. Since it squishes a large input space into a small input space between 0 and 1. Therefore, a large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9n2H6H0kkMf"
   },
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6stcdP1tlXN"
   },
   "source": [
    "Answer: Since the sigmoid and tanh activation functions are similar as their derivatives are between  0 to 0.25 and 0–1, the updated weight values are small, and the new weight values are very similar to the old weight values, thus leading to vanishing gradient problem! When inputs become very small or very large, the sigmoid function saturates at 0 and 1 and the tanh function saturates at -1 and 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruYJSIj8kkMf"
   },
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "  1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "  1. The gradient of ReLU is linear with its input when the input is positive.\n",
    "  1. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p7sWZaUuTtn"
   },
   "source": [
    "Answer: 1. true, since if the input in our model is positive, the ReLU function returns it, and if it is negative then it returns 0, and this because the ReLU’s derivative is 1 for values larger than zero as multiplying 1 by itself several times still gives 1 and the negative component of the ReLU function cannot be discriminated against because it is 0. As a result, negative values’ derivatives are simply set to 0. Thus never reaching a vanishing gradient \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns2gZicSxJoJ"
   },
   "source": [
    "2. Range of ReLU is [0, inf). ReLU activation function is continuous, but not differentiable at x = 0, as the derivative or gradient of ReLU has a constant value when x > 0The gradient of ReLU is 1 for \n",
    "x > 0 and 0 for x < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5sQ-Ik-1PhT"
   },
   "source": [
    "3. ReLU cannot learn on examples for which their activation is zero. it happens somtimes that is occurs sometimes when you initialize the entire neural network with zero and place ReLU on the hidden layers. Another cause is when a large gradient flows through, what will happen is that a ReLU neuron will update its weight and might be ended up with a big negative weight and bias. When this happens then this neuron will always produce 0 during the forward propagation, and then the gradient flowing through this neuron will forever be zero irrespective of the input, therefore the weights of this neuron will never be updated again which means its good as dead, thus calling it a \"dead\" neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzq_9PUxkkMf"
   },
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkHYysspkkMg"
   },
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: In regular gradient descent we run through all of the samples in the training set before doing a single update for a parameter. In stochastic gradient descent, we use only one training sample from the training set for parameter update.\n",
    "Minibatch Stochastic gradient descent is a mix of both, we run over a subset of the training samples before doing a parameter update.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAZZhLuhkkMg"
   },
   "source": [
    "2. Regarding SGD and GD:\n",
    "  1. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "  2. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: A. If the training set is very large, we will likely prefer to use SGD or minibatch SGD, since regular gradient descent may take very long, Whereas SGD and minibatch SGD will improve much faster since they don’t need to run through all samples before every update.\n",
    "\n",
    "If we have a good chance of reaching converging into local minimum, we will prefer to use SGD since it has a better chance of escaping it. That is because regular GD always runs on the same samples and SGD will run on different samples before each parameter update.\n",
    "\n",
    "B. Finding the closed-form solution is computationally complex, however, when we have only a small training set, it will be computationally reasonable to find the pseudo inverse function. So we will not need to use gradient descent at all since we will have a closed form solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-OxBNDpkkMg"
   },
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: I would expect the number of iterations to increase.\n",
    "Small batch sizes are noisy, so using them gives a regularization effect.\n",
    "Using a big batch size will cause us to train over similar samples in each iteration, increasing the generalization error. And we will be more likely to converge into a local minima and never escape it and reach the previous loss value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nExKIEaRkkMg"
   },
   "source": [
    "4. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "  1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "  1. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "  1. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "  1. Training  with SGD requires more memory than with GD.\n",
    "  1. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "  1. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: A. True, by definition, one run of an epoch means that every sample in the training set was taken into consideration for updating the loss function.\n",
    "\n",
    "B. False, gradients with SGD have more variance since they sample a different sample in every update, so the convergence will be very noisy and it is likely that they will avoid the global minima.\n",
    "\n",
    "C. True, since SGD samples a different sample every update and not the same batch of samples, so the convergence will be more noisy and it will be easier to escape a local minima.\n",
    "\n",
    "D. True, in normal GD we have to save the entire training set in the RAM, in SGD we only need to save one sample in each parameter update, and then we can release it and save the next one.\n",
    "\n",
    "E. False, SGD is more likely to escape a local minimum since it runs on a different sample of the training set in each update. Which GD is more likely to converge to a local minimum and not the global one, since it runs on the same batch of samples in each update.\n",
    "\n",
    "F. True, The momentum will help the gradient to continue moving towards the global minimum as it reaches the local minimum (the narrow ravine), since it will remember the direction in which it moved towards the ravine, and will attempt continue the trend of moving in that direction, even if the current samples the SGD is training on have a different gradient (depends on how different their gradient is and on the momentum hyperparameter).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SV-BE7BvkkMg"
   },
   "source": [
    "5. **Bonus** (we didn't discuss this at class):  We can use bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "  **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x81Xrcw9kkMg"
   },
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "  1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "  2. How can each of these problems be caused by increased depth?\n",
    "  3. Provide a numerical example demonstrating each.\n",
    "  4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: A. Vanishing gradients is a phenomenon that occurs when the gradient keeps getting smaller as we go backward during backpropagation. This causes the weights update to become very small and the neural network to stop improving.\n",
    "This happens due to the sigmoid and tanh activation functions since their values are below 1, so the derivative will always grow smaller.\n",
    "\n",
    "Exploding gradients occurs when the gradient keeps getting larger as we go backward through the layers. This happens due to high weights, they cause the gradient to also become high and therefore the new weights will differ a lot from the old ones. Which will cause us to start jumping and miss the global minimum.\n",
    "\n",
    "B. In the vanishing gradients problem, the larger the depth of our network, there are more times when the gradient gets smaller, until it is so small that the gradient becomes negligible and our loss function barely improves.\n",
    "\n",
    "Similarly, in the exploding gradient problem, the larger the depth, the more the gradient will increase, until it becomes so large that the gradient changes too much and no longer approaches the global minimum.\n",
    "\n",
    "C. Vanishing gradients – assume for example that our activation function is sigmoid, which is between 0 and 0.25, and that our weights have values less than 1.\n",
    "Therefore for many hidden layer we will multiply by less than 0.25 a large amount of times until the gradient becomes so small that the model almost stops learning.\n",
    "\n",
    "Exploding gradients – assume we use the RELU activation function, which is 0 for negative values and 1 for positive ones. And weights that have values higher than 10.\n",
    "Than for many hidden layers we will multiply by more than 10 a large amount of times until the gradient becomes so large that it will stop converging towards a minimum.\n",
    "\n",
    "D. We can look at how our loss function changes in each epoch, if it doesn’t seem to improve, then we most likely have the vanishing gradients problem.\n",
    "If the loss function changes a lot, but does not converge towards the global minimum, then we most likely have the exploding gradients problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdST0QkbkkMg"
   },
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGL_SAr2kkMg"
   },
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $$\n",
    "  \\hat{y}^{(i)} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}^{(i)}+ \\vec{b}_1) + \\vec{b}_2\n",
    "  $$\n",
    "  Your wish to minimize the in-sample loss function is defined as\n",
    "  $$\n",
    "  L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)\n",
    "  $$\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "  $$\n",
    "  \\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})\n",
    "  $$\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ym14cMe8Oq0H"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHzwYhbskkMg"
   },
   "source": [
    "2. The derivative of a function $f(\\vec{x})$ at a point $\\vec{x}_0$ is\n",
    "  $$\n",
    "  f'(\\vec{x}_0)=\\lim_{\\Delta\\vec{x}\\to 0} \\frac{f(\\vec{x}_0+\\Delta\\vec{x})-f(\\vec{x}_0)}{\\Delta\\vec{x}}\n",
    "  $$\n",
    "  \n",
    "  1. Explain how this formula can be used in order to compute gradients of neural network parameters numerically, without automatic differentiation (AD).\n",
    "  \n",
    "  2. What are the drawbacks of this approach? List at least two drawbacks compared to AD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDXC-Cl-dRfJ"
   },
   "source": [
    "Answer: 1. \n",
    "Numerical differentiation is powerful tool to check the correctness of implementation, \n",
    "The derivative of a function f(x) measures the sensitivity to change of the function value (output value) with respect to a change in its argument x (input value). In other words, the derivative tells us the direction f(x) is going.\n",
    "The gradient shows how much the parameter x needs to change (in positive or negative direction) to minimize f(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d8x3eXYmJvv"
   },
   "source": [
    "2. rounding error, and slow to compute, and requires to keep intermediate data in the memory during the forward pass in case it will be used in the backpropagation. In addition, it has a lack of flexibility, e.g., compute the gradient of gradient. Whereas AD create computation graph for gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2vohV0nkkMg"
   },
   "source": [
    "3. Given the following code snippet:\n",
    "  1. Write a short snippet that implements that calculates gradient of `loss` w.r.t. `W` and `b` using the approach of numerical gradients from the previous question.\n",
    "  2. Calculate the same derivatives with autograd.\n",
    "  3. Show, by calling `torch.allclose()` that your numerical gradient is close to autograd's gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.049487Z",
     "iopub.status.busy": "2021-01-26T09:18:18.048852Z",
     "iopub.status.idle": "2021-01-26T09:18:18.072993Z",
     "shell.execute_reply": "2021-01-26T09:18:18.073616Z"
    },
    "id": "qCP0QFXOkkMg",
    "outputId": "044976ea-6be5-4106-b604-9b8a2138d24f"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<fstring>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (loss=)\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, d = 100, 5\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d, dtype=dtype)\n",
    "W, b = torch.rand(d, d, requires_grad=True, dtype=dtype), torch.rand(d, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def foo(W, b):\n",
    "    return torch.mean(X @ W + b)\n",
    "\n",
    "loss = foo(W, b)\n",
    "print(f\"{loss=}\")\n",
    "\n",
    "# TODO: Calculate gradients numerically for W and b\n",
    "# grad_W =...\n",
    "# grad_b =...\n",
    "\n",
    "# TODO: Compare with autograd using torch.allclose()\n",
    "# autograd_W = ...\n",
    "# autograd_b = ...\n",
    "# assert torch.allclose(grad_W, autograd_W)\n",
    "# assert torch.allclose(grad_b, autograd_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGN0oBDXkkMg"
   },
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l7_IwHbkkMh"
   },
   "source": [
    "1. Regarding word embeddings:\n",
    "  1. Explain this term and why it's used in the context of a language model.\n",
    "  1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding (i.e. trained directly on sequences of tokens)? If yes, what would be the consequence for the trained model? if no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: A. Word embeddings are representations of text where words that have similar meaning  have a similar representation.\n",
    "The encoding of a word is usually a vector that encodes the word such that the distance between two vectors that represent words is small if the words are similar and vice versa.\n",
    "A great advantage of word embeddings as opposed to other ways of encoding text is that for a large vocabulary, we will not necessarily have sparse vectors that result in more memory resources, since not every word must correspond to a number in the vector, so we can use a more expressive representation of the text.\n",
    "Language models can take advantage of these embeddings by modifying them to allow context dependent embeddings, where the same word may have a different embedding in different sentences.\n",
    "\n",
    "B. Since the vocabulary in the sentiment analysis model is so large, training it directly on sequences of tokens will require us to have sparse vectors of high dimensionality, which may cause significant performance issues. In addition, using word embeddings will allow word with similar meaning to have a similar representation, which will increase the accuracy of our model. So a language model like the sentiment analysis can be trained without an embedding, but with lower performance and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPSharnskkMh"
   },
   "source": [
    "2. Considering the following snippet, explain:\n",
    "  1. What does `Y` contain? why this output shape?\n",
    "  2. **Bonus**: How you would implement `nn.Embedding` yourself using only torch tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: X is a tensor of shape (5, 6, 7, 8) filled with integers between 0 and 42.\n",
    "embedding is an embedding module containing 42 tensors of size 42000 (embedds 42 tokens to vectors of dimension 42000).\n",
    "The size of the embedding is 42 since we have 42 different possible integers in X, and we want to embed each of them to a different vector.\n",
    "\n",
    "So Y is a Tensor that embedds each integer with a different value in X to a vector of dimension 42000. (two integers with the same value will be mapped to the same vector)\n",
    "It’s shape is (5, 6, 7, 8, 42000) since each integer in the origin tensor X, which is of shape (5, 6, 7, 8) will now be a 42000 dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-26T09:18:18.077958Z",
     "iopub.status.busy": "2021-01-26T09:18:18.077299Z",
     "iopub.status.idle": "2021-01-26T09:18:18.378980Z",
     "shell.execute_reply": "2021-01-26T09:18:18.379850Z"
    },
    "id": "hma99gF6kkMh",
    "outputId": "1f7e3944-3254-44a0-a88b-3be0a140f1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape=torch.Size([5, 6, 7, 8, 42000])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6rIH_YFkkMh"
   },
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of S: State whether the following sentences are **true or false**, and explain.\n",
    "  1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "  2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length S.\n",
    "  3. TBPTT allows the model to learn relations between input that are at most S timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: A. True: The backpropagation algorithm presents a sequence of timesteps of input and output pairs to the network, calculates errors in each timestep and updates the weights. Truncated backpropagation is a modified version since it processes the sequence one timestep at a time, but only S times during the algorithm the algorithm will calculate error across S timesteps. This makes training large sequences more efficient since we can consider them as a number of smaller sequences instead of having to go through the entire sequence for a single parameter update.\n",
    "\n",
    "B. False: To implement TBPTT, we don’t limit the sequence model to length S, but we limit the interval between each run of BPTT and the number of timesteps we apply BPTT on to S. The sequence model length will remain the same.\n",
    "\n",
    "C. True: The model only calculates errors every S timesteps, on the previous S timesteps, it will not consider the timesteps that came before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vutzRwE5kkMh"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPsYA6U5kkMh"
   },
   "source": [
    "1. In tutorial 5 we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "  1. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention?\n",
    "  \n",
    "  2. After learning that self-attention is gaining popularity thanks to the transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections, like in the tutorial..). What influence do you expect this will have on the learned hidden states?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: A. The attention mechanism creates a unique mapping between each time step of the decoder output to all the hidden states.\n",
    "This way we are able to utilize all of the hidden states of the input sequence during the decoding process, this allows the decoder to focus more on hidden states that are important for it and less on unimportant ones.\n",
    "\n",
    "The model without attention only passes the hidden state of the previous input, using this model we have to save the complete sequence of information using a single vector.\n",
    "Therefore, the attention model allows us to accurately process long input sequences, by remembering all of the hidden states and not just the last one.\n",
    "\n",
    "B. When using self-attention, each hidden state will keep the dependencies between each word in the sequence to every other word in the same sequence, and this way the relationship between words in the sequence are captured.\n",
    "Since we no longer need to remember all of the previous hidden states, we expect to do a better job performance wise.\n",
    "Since we keep dependencies between each word in the sequence, we will likely do better in understanding the dependencies between different parts of the sequence, the syntactic function between words in the sentence.\n",
    "However since we no longer keep all of the previous hidden states, we might do worse in long sequences that have a large dependency between words that are very far from each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pl5AJtKrkkMh"
   },
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za0_XxK4kkMh"
   },
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "  1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "  1. Images generated by the model ($z \\to x'$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Without the KL-divergence term, we would have no regularization, our distributions will not be consistent, so the model might ignore the fact that distributions are returned and behave like a normal autoencoder.\n",
    "A. There will be no difference in the reconstruction\n",
    "B. The latent space will have a less consistent distribution which will cause the model to have a much harder time generating accurate images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxei24SkkkMh"
   },
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  2. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction.\n",
    "  3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "A. False, It is true that we attempt to keep the latent space close to the N(0, 1) distribution. But out distribution will eventually be N(U, σ), where U and σ are decided during training.\n",
    "\n",
    "B. False, when sampling for the VAE we use the reparametrization trick where we sample from an isotropic Gaussian. This proccess will lead to different z values. These z values will give different reconstruction when passed to the decoder.\n",
    "    \n",
    "C.  True, we maximize ELBO which in term minimizes the loss function which is intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NGMw1g6kkMh"
   },
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "  1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "  2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "  3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "  4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "  5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "A) True, when training is complete we want the generator to fool the discriminator. This means that its loss is low and the discriminators loss is high because it can't distinguish between real images and fake ones.\n",
    "B)False, when training the discrininator we do not train the generator.\n",
    "C) True, when generating an image we sample random noise from the standard normal distribution and feed it to the generator.\n",
    "D) True, if the discriminator is trained it can help the generator train by giving a higher loss value which will help back propogation. It is important to note that a we should not train it to much because then it can always detect fake images and the generator will not learn.\n",
    "E) False, when this state is reached there is no reason to continue training the generator because the discriminator is just making random guesses. It will be equivalent to training the model on a coin flip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2rUbii5kkMh"
   },
   "source": [
    "### Detection and Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuhzroWQkkMh"
   },
   "source": [
    "1. What is the diffrence between IoU and Dice score? what's the diffrance between IoU and mAP?\n",
    "    shortly explain when would you use what evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The main difference between IoU and Dice score comes when taking the average score over a set of inferences.\n",
    "The difference emerges when trying to decide how much worse classifier B is than A for any given case.\n",
    "IoU score tends to penalize single instances of bad classification more than Dice score, even when they can both agree that instance is bad.\n",
    "So the Dice score tends to measure closer to average performance, while the IoU score measures closer to the worst case performance.\n",
    "\n",
    "For this reason, we will prefer to use IoU when we have images with a small amount of objects to classify, and we want to make sure that we classify all of them, even if not at the exact location. We will prefer to use Dice in images with many objects where we care less about missing a single object as long as the general accuracy is high.\n",
    "\n",
    "mAP is a bit different, since it does not care how accurate it was when detecting an object, but rather it uses IoU to determine how many of the objects were correctly classified and how many were classified incorrectly. (We define an IoU threshold, if the IoU score is higher than it, it was classified correctly and vice versa).\n",
    "\n",
    "So it will make sense to use mAP when we want to know how many of each object where in a picture, but not necessarily their accurate location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCLvQV5AkkMh"
   },
   "source": [
    "2. regarding of YOLO and mask-r-CNN, witch one is one stage detector? describe the RPN outputs and the YOLO output, adress how the network produce the output and the shapes of each output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: YOLO is a one stage detector, since it requires only a single forward propagation through a neural network to detect objects. \n",
    "MASK – R – CNN is a two stage detector since it uses one model to extracts regions of objects, and a second model to classify them.\n",
    "\n",
    "RPN – region proposal network: the model outputs a set of proposals, each has a score of it’s probability of being an object and the label of the object.\n",
    "To generate these proposals for the region where the object lies, a small network is slide over a convolutional feature map that is the output of the last convolutional layer.\n",
    "RPN has a classifier and a regressor: the classifier determines the probability of a proposal having the target object, and the regressor determines the coordinates of the proposals.\n",
    "We choose a number of anchors K (usually 9), there will be a total of k possible proposals for each pixel.\n",
    "The anchors are assigned labels based on their IoU scores and we output the set of proposals, which for an image of width W and height H, will be a Tensor of shape [W, H, K].\n",
    "\n",
    "Yolo – first, the image is divided into grids, every grid cell will detect objects that appear within them.\n",
    "We then use bounding box regression to predict the locations and labels of objects in the cells.\n",
    "Then we use IoU to ensure that the predicted bounding boxes are equal to the real boxes of the objects. We can than eliminate unnecessary bounding boxes so that the final detection will only consist of the best ones.\n",
    "So, the final output of YOLO are bounding boxes that detect objects.\n",
    "If the size of each cell is equal to (S x S), we have B bounding boxes in each cell, and each bounding box has 5 attributes (x, y, w, h) and a box confidence score. and there are K possible labels.\n",
    "Then the shape of the output will be equal to (S, S, B * 5 + K).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part3_SummaryQuestions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
